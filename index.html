<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<link href="https://fonts.googleapis.com/css2?family=Open+Sans&display=swap"
      rel="stylesheet">
<link rel="stylesheet" type="text/css" href="/style.css" media="screen"/>

<html lang="en">
<head>
  	<title>NOAH</title>
      <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/
          if you update and want to force Facebook to re-scrape. -->
  	<meta property="og:image" content="/teaser.png"/>
  	<meta property="og:title" content="WorldQA: Multimodal World Knowledge in Videos through Long-Chain Reasoning" />
  	<meta property="og:description" content="Multimodal World Knowledge in Videos through Long-Chain Reasoning" />
    <!-- Twitter automatically scrapes this. Go to https://cards-dev.twitter.com/validator?
        if you update and want to force Twitter to re-scrape. -->
    <meta property="twitter:card"          content="summary" />
    <meta property="twitter:title"         content="WorldQA" />
    <meta property="twitter:description"   content="Multimodal World Knowledge in Videos through Long-Chain Reasoning" />
    <meta property="twitter:image"         content="/teaser.png" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Add your Google Analytics tag here -->
    <!-- <script async
            src="https://www.googletagmanager.com/gtag/js?id=UA-97476543-1"></script> -->
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());
        gtag('config', 'UA-97476543-1');
    </script>

</head>

<body>
<div class="container">
    <div class="title">
       WorldQA
    </div>

    <!-- <div class="venue">
        arXiv
    </div> -->

    <br><br>

    <div class="author">
        <a href="https://zhangyuanhan-ai.github.io/">Yuanhan Zhang</a><sup>1</sup>
    </div>

    <div class="author">
        <a href="https://www.linkedin.com/in/kaichen-zhang-014b17219/?originalSubdomain=sg">Kaichen Zhang</a><sup>1</sup>
    </div>

    <div class="author">
        <a href="https://brianboli.com/">Bo Li</a><sup>1</sup>
    </div>

    <div class="author">
        <a href="https://pufanyi.github.io/">Fanyi Pu</a><sup>1</sup>
    </div>

    <div class="author">
        <a href="https://www.linkedin.com/in/casetiadharma/?originalSubdomain=sg">Christopher Arif Setiadharma</a><sup>1</sup>
    </div>

    <div class="author">
        <a href="https://jingkang50.github.io/">Jingkang Yang</a><sup>1</sup>
    </div>

    <div class="author">
        <a href="https://liuziwei7.github.io/">Ziwei Liu</a><sup>1</sup>
    </div>

    <br><br>

    <div class="affiliation"><sup>1&nbsp;</sup>S-Lab, Nanyang Technological University</div>

    <br><br>

    <div class="links"><a href="https://arxiv.org/pdf/2405.03272">[Paper]</a></div>
    <div class="links"><a href="https://www.youtube.com/watch?v=NXbJLLf9E_I">[Demo]</a></div>
    <div class="links"><a href="https://github.com/EvolvingLMMs-Lab/lmms-eval/tree/kc/worldqa">[Code]</a></div>


    <br><br>

    <!-- <table class="center" style="width: 100%;margin-right:auto;margin-left:auto;" > -->

    <!-- </table> -->

    <!-- <img style="width: 80%;" src="/teaser.png" alt="Teaser figure."/> -->
    <!-- <br>
    <p style="width: 80%;">
        This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful project</a>, and inherits the modifications made by <a href="https://github.com/jasonyzhang/webpage-template">Jason Zhang</a>.
        The code can be found <a href="https://github.com/elliottwu/webpage-template">here</a>.
    </p> -->



    <h1>Abstract</h1>
    <p style="width: 80%;">
        Multimodal information, together with our knowledge, help us to understand the complex and dynamic world. Large language models (LLM) and large multimodal models (LMM), however, still struggle to emulate this capability. In this paper, we present <strong>WorldQA</strong>, a video understanding dataset designed to push the boundaries of multimodal world models with three appealing properties: <strong>(1) Multimodal Inputs:</strong> The dataset comprises 1007 question-answer pairs and 303 videos, necessitating the analysis of both auditory and visual data for successful interpretation. <strong>(2) World Knowledge:</strong> We identify five essential types of world knowledge for question formulation. This approach challenges models to extend their capabilities beyond mere perception.  <strong>(3) Long-Chain Reasoning:</strong>Our dataset introduces an average reasoning step of 4.45, notably surpassing other videoQA datasets. Furthermore, we introduce <strong>WorldRetriever</strong>, an agent designed to synthesize expert knowledge into a coherent reasoning chain, thereby facilitating accurate responses to WorldQA queries. Extensive evaluations of 13 prominent LLMs and LMMs reveal that WorldRetriever, although being the most effective model, achieved only 70% of humanlevel performance in multiple-choice questions. This finding highlights the necessity for further advancement in the reasoning and comprehension abilities of models. Our experiments also yield several key insights. For instance, while humans tend to perform better with increased frames, current LMMs, including WorldRetriever, show diminished performance under similar conditions. We hope that WorldQA,our methodology, and these insights could contribute to the future development of multimodal world models.
    </p>

    <br><br>
    <hr>

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/watch?v=NXbJLLf9E_I&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->

    <h1>WorldQA</h1>
    <p style="width: 80%;">
       <img src="resources/data.png" alt="Descriptive text about the image">
    </p>
    <p style="width: 80%;">
        WorldQA presents significant advancements over existing datasets, as (a) illustrates. Firstly, it requires complex multi-step reasoning. Using GPT4, we evaluated the reasoning steps in each question-answer pair across datasets; WorldQA averages 4.45 steps, notably higher than others which typically involve less than two steps. This complexity is also evident in answer lengths: while answers in other VideoQA datasets average below five words, those in WorldQA average 24.3 words. Secondly, it necessitates more than visual information for success. WorldQA encompasses audio comprehension and world knowledge, expanding its scope beyond mere video visuals for effective
        question resolution. To our knowledge, it represents the first
        VideoQA dataset that incorporates questions necessitating
        world knowledge.
    </p>


    <h1>WorldRetriever</h1>
    <p style="width: 80%;">
       <img src="resources/WorldRetriever.png" alt="Descriptive text about the image">
    </p>
    <p style="width: 80%;">
        we introduce WorldRetriver, a method that leverages LLM-as-agent for complex long-chain reasoning. As depicted in Fig. 1, human long-chain reasoning involves gathering information from various sensors and integrating world knowledge to reach a conclusion. WorldRetriver mimics this approach by using expert models for individual sub-tasks. These models perform specific functions, and then WorldRetriver combines their outputs with the original question to formulate the final answer.
    </p>

    <h1>Paper</h1>
    <div class="paper-thumbnail">
        <a href="https://arxiv.org">
            <img class="layered-paper-big" width="100%" src="/paper.png" alt="Paper thumbnail"/>
        </a>
    </div>
    <div class="paper-info">
        <h3>WorldQA: Multimodal World Knowledge in Videos through Long-Chain Reasoning</h3>
        <p>Yuanhan Zhang, Kaichen Zhang, Bo Li, Fanyi Pu, Christopher Arif Setiadharma, Jingkang Yang, Ziwei Liu</p>
        <p>arXiv, 2024.</p>
        <pre><code>@InProceedings{zhang2024WorldQA,
    title = {WorldQA: Multimodal World Knowledge in Videos through Long-Chain Reasoning},
    author = {Yuanhan Zhang, Kaichen Zhang, Bo Li, Fanyi Pu, Christopher Arif Setiadharma, Jingkang Yang and Ziwei Liu},
    archivePrefix={arXiv},
    year = {2024},
}</code></pre>
    </div>

    <br><br>
    <hr>

    <!-- <h1>Acknowledgements</h1>
    <p style="width: 80%;">
        TBA.
    </p> -->

    <br><br>
</div>

</body>

</html>
